{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Assignment_2_deep_learning_models_for_sentiment_classification (2) (1).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subir357/NLP/blob/main/Assignment_2_deep_learning_models_for_sentiment_classification_(2)_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8o4w70-5wpf"
      },
      "source": [
        "# Deep learning models for sentiment classification\n",
        "\n",
        "### Full marks: 10\n",
        "\n",
        "In this assignment you will implement a few simple deep learning models for sentiment classification using review data. \n",
        "\n",
        "You are required to implement the functions as instructed. <b>Do not change the signatures of the functions.</b>\n",
        "\n",
        "<b>Deadline: Monday, 16 August 2021 03:00 AM</b>\n",
        "\n",
        "<i>This is an easy assignment. An extension of this deadline should not be required.</i>\n",
        "\n",
        "<b>Submission instruction</b>:\n",
        "Submit an ipynb file to the classroom. If you work on Colab, download your file as ipynb and upload that file. Name your file suffixed by your roll number for convenience.\n",
        "\n",
        "<b>Important</b>: Do not just share the link of your colab file. Since the assignment needs to be submitted before the deadline, the file you submit should not be editable by you after you submit. A mistake in this regard will not be acceptable under any circumstances, it will be equivalent to not submitting the assignment.\n",
        "\n",
        "<b>Late submission penalty</b>: For every hour of being late, you will be charged 1% of the marks you get. So, if you submit 2 days late, you lose almost half of the marks. Please do not keep this as a last minute task, you will need time to solve it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgs3G7uX0Y6P"
      },
      "source": [
        "## Setup \n",
        "\n",
        "Get the auth code using your Google account so that you can read the files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT4mQXDO7GL8"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()  \n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zMEtmKT7jX2"
      },
      "source": [
        "# Training data (anyone on the internet can view)\n",
        "# https://drive.google.com/file/d/1E4aQmxQg_RLHVH28G2sUecUQwvGl0EXE/view?usp=sharing \n",
        "\n",
        "# Test data (anyone on the internet can view)\n",
        "# https://drive.google.com/file/d/1i0nX9LxYI1oh7872qXTVrVqliQCnkbOn/view?usp=sharing\n",
        "\n",
        "filename_train = 'reviews_train.bz2'\n",
        "filename_test = 'reviews_test.bz2'\n",
        "\n",
        "id_train = '1E4aQmxQg_RLHVH28G2sUecUQwvGl0EXE' \n",
        "id_test = '1i0nX9LxYI1oh7872qXTVrVqliQCnkbOn' \n",
        "\n",
        "train_file = drive.CreateFile({'id': id_train})\n",
        "test_file = drive.CreateFile({'id': id_test})\n",
        "\n",
        "# Read the files\n",
        "train_file.GetContentFile(filename_train)\n",
        "test_file.GetContentFile(filename_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_6fUQJ1PNU"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Some essential imports are listed below. You may import more libraries if you want to. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-06-22T03:03:06.184429Z",
          "iopub.execute_input": "2021-06-22T03:03:06.185019Z",
          "iopub.status.idle": "2021-06-22T03:03:08.077978Z",
          "shell.execute_reply.started": "2021-06-22T03:03:06.18496Z",
          "shell.execute_reply": "2021-06-22T03:03:08.077175Z"
        },
        "trusted": true,
        "id": "k1n30ADd5wph"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import bz2\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrZ3uMWN5wpi"
      },
      "source": [
        "## 1. Read texts and labels from the files (1 mark)\n",
        "\n",
        "Each line in the given files starts with <tt>\\_\\_label\\_\\_1</tt> or <tt>\\_\\_label\\_\\_2</tt> (two underscores, 'label', two underscores and  then 1 or 2), followed by a whitespace and then the text. \\_\\_label\\_\\_1 indicates <b>negative sentiment</b> and \\_\\_label\\_\\_2 indicates <b>positive sentiment</b>. \n",
        "\n",
        "Write a function to read the file and return labels (numpy array of integers) and texts (array or list of strings). Ideally, you want to convert positive sentiment labels to 1 and negative sentiment labels to 0 right here. If you don't, you will have to deal with it later in the model. \n",
        "\n",
        "\n",
        "If you want, you may also apply some preprocessing to the text (e.g., lowercase, stemming, remove punctuations, etc), but it is optional. Note that not all such preprocessing steps may help in sentiment classification. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "execution": {
          "iopub.status.busy": "2021-06-22T03:03:16.592669Z",
          "iopub.execute_input": "2021-06-22T03:03:16.59299Z",
          "iopub.status.idle": "2021-06-22T03:06:21.158185Z",
          "shell.execute_reply.started": "2021-06-22T03:03:16.592953Z",
          "shell.execute_reply": "2021-06-22T03:06:21.156987Z"
        },
        "trusted": true,
        "id": "5turqrm15wpj"
      },
      "source": [
        "def read_texts_and_labels(file):\n",
        "  labels={}\n",
        "  texts={}\n",
        "  i=0\n",
        "  for line in bz2.BZ2File(file):\n",
        "    x = line.decode(\"utf-8\")\n",
        "    # Implement the rest and return appropriately\n",
        "    texts[i]=x[11:].lower()\n",
        "\n",
        "    all_words = x.split()\n",
        "    first_word= all_words[0]\n",
        "    if first_word ==\"__label__2\":\n",
        "      first_word=1\n",
        "      labels[i]=1\n",
        "    else:\n",
        "      labels[i]=0\n",
        "    i=i+1\n",
        "  return texts,labels\n",
        "  \n",
        "train_texts, train_labels = read_texts_and_labels(filename_train)\n",
        "test_texts, test_labels = read_texts_and_labels(filename_test)\n",
        "\n",
        "train_texts=list(train_texts.values())\n",
        "train_labels=list(train_labels.values())\n",
        "test_texts=list(test_texts.values())\n",
        "test_labels=list(test_labels.values())\n",
        "#categorical\n",
        "test_labels=keras.utils.to_categorical(test_labels,num_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "zVRlb0QF5wpk"
      },
      "source": [
        "## 2. Set aside validation data (1 mark)\n",
        "\n",
        "Since there are a lot of data points, set aside 5% data as validation set. You may use the <tt>train_test_split</tt> function from <tt>sklearn</tt>, or any other method to do it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wSHkNxzc5wpl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts_split, val_texts_split, train_labels_split, val_labels_split=train_test_split(train_texts, train_labels, test_size=0.05, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUemqLDx5wpm"
      },
      "source": [
        "## 3. Convert the texts to sequences and pad the sequences (1 mark)\n",
        "\n",
        "Convert the texts (train\\_text, val\\_text and test\\_text) to sequences using keras tokenizer. After that, pad the sequences with leading zeros using keras. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RxhqDxu15wpm"
      },
      "source": [
        "vocab_size=3000\n",
        "trunc_type='post'\n",
        "oov_tok='<oov>'\n",
        "tokenizer= Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "word_index=tokenizer.word_index\n",
        "sequences=tokenizer.texts_to_sequences(train_texts_split)\n",
        "val_texts_tokenized= tokenizer.texts_to_sequences(val_texts_split)\n",
        "test_texts_tokenized=tokenizer.texts_to_sequences(test_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "H7aeIP1w5wpn"
      },
      "source": [
        "max_length_of_sequences = 100\n",
        "\n",
        "\n",
        "train_texts_split_tokenized_padded=pad_sequences(sequences, maxlen=max_length_of_sequences, truncating=trunc_type)\n",
        "val_texts_tokenized_padded=pad_sequences(val_texts_tokenized, maxlen=max_length_of_sequences, truncating=trunc_type)\n",
        "test_texts_tokenized_padded=pad_sequences(test_texts_tokenized, maxlen=max_length_of_sequences, truncating=trunc_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgBOa5qpKI6E"
      },
      "source": [
        "### Scale of padded_sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY2FZcL0KiuM"
      },
      "source": [
        "normalizer_train_texts_split_tokenized_padded=train_texts_split_tokenized_padded/3000 \n",
        "val_texts_tokenized_padded=val_texts_tokenized_padded/3000 \n",
        "normalizer_test_texts_tokenized_padded=test_texts_tokenized_padded/300 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAoYrqrVwWM0"
      },
      "source": [
        "normalizer_test_texts_tokenized_padded=test_texts_tokenized_padded/10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ_TlmTu5wpn"
      },
      "source": [
        "## 4. A simple feedforward network (2 marks)\n",
        "\n",
        "Implement a simple feedforward network with one embedding layer with <tt>d1</tt> dimension, then one hidden layer with <tt>d2</tt> dimension (activation function ReLU) and an output layer with appropriate dimension and activation function. \n",
        "\n",
        "Then, compile the model with a suitable optimizer and suitable loss and metrics. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXlz_6bCdFx3"
      },
      "source": [
        "# You may tune these hyperparameters, but implement the model as instructed above.\n",
        "d1 = 200\n",
        "d2 = 128 \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,Dense\n",
        "\n",
        " # implement the model and return the model\n",
        "model= tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=3000, output_dim=32, input_length=100))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "\n",
        "model1 = model\n",
        "optimizer=keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(\n",
        "    # Implement this code\n",
        "    optimizer=optimizer,loss='binary_crossentropy',metrics=['acc']\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1Vfc4dWTUeZ"
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG6ThcLBMb8g"
      },
      "source": [
        "\n",
        "train_labels_split=keras.utils.to_categorical(train_labels_split,num_classes=2)\n",
        "val_labels_split=keras.utils.to_categorical(val_labels_split,num_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajA86KRKu5cF"
      },
      "source": [
        "Train the model using the fit() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7l0oci8_5wpo"
      },
      "source": [
        "model.fit(normalizer_train_texts_split_tokenized_padded, train_labels_split, batch_size=256, epochs=3,validation_data=(val_texts_tokenized_padded, val_labels_split))\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1mbAnjP5wpp"
      },
      "source": [
        "## 4A. Experiment and note your observation (1 mark)\n",
        "\n",
        "Experiment with the hyperparameters a bit (dimensions, learning rate, optimizer) and note your observations in the cell below. You do not have to experiment with all possible choices of hyperparameters, but also not only one choice.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUve9aBZvqGM"
      },
      "source": [
        "WRITE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEOvSdEJ5wpq"
      },
      "source": [
        "## 5. Implement a birectional GRU Model (2 marks)\n",
        "\n",
        "Implement a GRU model with the following details:\n",
        "\n",
        "*   First, an embedding layer with <tt>d1</tt> dimension.\n",
        "*   Then, a bidirectional GRU layer with <tt>d2</tt> dimension.\n",
        "*   Then, a unidirectional GRU layer with <tt>d3</tt> dimension.\n",
        "*   Lastly, an appropriate output layer with activation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ieefb9l45wpq"
      },
      "source": [
        "model= tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=3000, output_dim=d1, input_length=20))\n",
        "model.add(keras.layers.Bidirectional(keras.layers.GRU(d2, return_sequences=True))\n",
        "model.add(keras.layers.BGRU(d3, return_sequences=True)\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model2 = model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6Nh6xzYnrIL"
      },
      "source": [
        "## 5A. Experiment and note your observation (1 mark)\n",
        "\n",
        "Experiment with the hyperparameters a bit (dimensions, learning rate, optimizer) and note your observations in the cell below. You do not have to experiment with all possible choices of hyperparameters, but also not only one choice.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DS0dHhA95wpr"
      },
      "source": [
        "model2 = model_GRU()\n",
        "model2.compile(\n",
        "    # Implement this code\n",
        ")\n",
        "\n",
        "model2.fit(\n",
        "    # complete code here\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeOMebQ-5wpr"
      },
      "source": [
        "## 6. Compute the accuracy and F1 scores (1 mark)\n",
        "\n",
        "Compute the accuracy and F1 scores for both models on the <b>test data</b>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4ByyniYh5wpr"
      },
      "source": [
        "# Implement code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKmGXUVM5wpr"
      },
      "source": [
        "## Optional (ungraded)\n",
        "\n",
        "Learn how to load a pre-trained BERT model and fine-tune that to classify the sentiment of the texts. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESaT81cRRWj8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}